{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffba0394-e41a-48b2-8447-9559de79a32e",
   "metadata": {
    "id": "ad909418-3384-444f-a7b6-f85a80eefa27"
   },
   "source": [
    "    # Assignment 2\n",
    "\n",
    "This assignment serves as a comprehensive evaluation of your machine learning skills, encompassing not only the technical aspects of model development but also your ability to analyze, interpret, and present data insights effectively. As such, it's essential to ensure that your submission is complete, functional, and devoid of any obvious gaps, as if you were delivering this project to a client.\n",
    "\n",
    "To achieve this, leverage the full capabilities of Markdown and the interactive visualization tools available in Jupyter notebooks to craft a well-structured and visually appealing report of your findings. Your report should clearly communicate the insights you've gained from the exploratory data analysis, the rationale behind your data preprocessing and feature engineering decisions, and a thorough analysis of feature importance. High-quality visualizations and well-organized documentation will not only support your analysis but also make your results more accessible and understandable to your audience.\n",
    "\n",
    "Remember, the ability to present complex results in an intuitive and engaging manner is a crucial skill, almost as important as the technical proficiency in model building and data analysis. Treat this assignment as an opportunity to showcase your skills in both areas.\n",
    "\n",
    "## Instructions\n",
    "- Your submission should be a `.ipynb` file with your name,\n",
    "  like `FirstnameLastname.ipynb`. It should include the answers to the questions in markdown cells, your data analysis and results.\n",
    "- You are expected to follow the best practices for code writing and model\n",
    "training. Poor coding style will be penalized.\n",
    "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
    "Plagiarism in the code will result in failing. If you use code from the\n",
    "internet, cite it by adding the source of the code as a comment in the first line of the code cell. [Academic misconduct policy](https://wiki.innopolis.university/display/DOE/Academic+misconduct+policy)\n",
    "- In real life clients can give unclear goals or requirements. So, if the instructions seem vague, use common sense to make reasonable assumptions and decisions.\n",
    "\n",
    "## Self-Reliance and Exploration\n",
    "In this task, you're encouraged to rely on your resourcefulness and creativity. Dive into available resources, experiment with various solutions, and learn from every outcome. While our team is here to clarify task details and offer conceptual guidance, we encourage you to first seek answers independently. This approach is vital for developing your problem-solving skills in machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d3afb-58f8-4335-8fac-074db8adeaa7",
   "metadata": {
    "id": "f38d3afb-58f8-4335-8fac-074db8adeaa7"
   },
   "source": [
    "# Task 2: Image Classification with CNNs (50%)\n",
    "\n",
    "In this task, you'll dive into the world of Convolutional Neural Networks (CNNs) by working with the CIFAR-10 dataset, a staple in image classification challenges. Your goal is to build and evaluate two different CNN models to classify images into one of the ten categories accurately.\n",
    "\n",
    "The dataset is availabel in pytorch and keras.\n",
    "\n",
    "## Part 1: Custom CNN Model (20%)\n",
    "\n",
    "- Design and train a CNN model from scratch tailored for the CIFAR-10 dataset.\n",
    "- Focus on the architecture that you believe will perform best for this specific task.\n",
    "- Integrate various techniques such as batch normalization, dropout, learning rate schedulers, and early stopping to improve model training. Experiment with these methods and finetune them to see how they affect training stability, convergence speed, and overall performance.\n",
    "\n",
    "## Part 2: Transfer Learning Model (20%)\n",
    "\n",
    "- Implement a transfer learning approach using a pre-trained model of your choice.\n",
    "- Fine-tune the model on the CIFAR-10 dataset to achieve the best possible performance.\n",
    "\n",
    "## Evaluation (10%)\n",
    "\n",
    "Ensure that both models are robust and generalized well to unseen data.\n",
    "\n",
    "After training both models, you will evaluate them on a provided test dataset.\n",
    "\n",
    "Compare your models based on:\n",
    "- **AUC-ROC**: How well does each model discriminate between classes?\n",
    "- **Model Size**: Consider the trade-offs in model complexity.\n",
    "- **Inference Speed**: Evaluate how quickly your model can predict classes for new images.\n",
    "\n",
    "Reflect on the performance, size, and inference speed of both models. What insights can you draw from these comparisons?\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand and apply CNNs for image classification.\n",
    "- Explore the impact of model architecture on performance and efficiency.\n",
    "- Learn the process and benefits of transfer learning in deep learning.\n",
    "\n",
    "Remember, the key to this task is not just about achieving the highest accuracy but also understanding the strengths and limitations of different approaches in machine learning model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94cdbd8-79b5-43ac-b0e1-0858da622e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:16.328489900Z",
     "start_time": "2024-04-23T12:36:12.737800800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e94cdbd8-79b5-43ac-b0e1-0858da622e6e",
    "outputId": "3c5d8051-f42d-4c04-b184-10af9ae22738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 code\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "# We need this transofrmations to make our model more generalized on the unknown data and avoid overfitting\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=2),# add zeros to save initial size of an image after transformations\n",
    "    transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "    transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# We do not modify test transforms, because it will corrupt test data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# Load train data\n",
    "train_dataset = datasets.CIFAR10(root='cifar10',\n",
    "                                 train=True,\n",
    "                                 transform=train_transforms,\n",
    "                                 download=True)\n",
    "\n",
    "train_data_loader = data.DataLoader(train_dataset,\n",
    "                                    batch_size=train_batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=2)\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_dataset = datasets.CIFAR10(root='cifar10',\n",
    "                                 train=False,\n",
    "                                 transform=test_transforms,\n",
    "                                 download=True)\n",
    "\n",
    "test_data_loader = data.DataLoader(test_dataset,\n",
    "                                    batch_size=test_batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=2)\n",
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e1ded9245ec610d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:23.504177Z",
     "start_time": "2024-04-23T12:36:23.474154700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e1ded9245ec610d",
    "outputId": "dc7928bd-cf4b-4040-99fb-8a7db6f38b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Cifar10_model(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear1): Sequential(\n",
      "    (0): Linear(in_features=7744, out_features=256, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Let's build a model\n",
    "# Lab10 code, but I change activation function to LeakyRelu. It allows to avoid gradient vanishing.\n",
    "# for Rely everything < 0 is 0, but leaky Relu assigns small values for these variables\n",
    "# Reference: https://www.baeldung.com/cs/relu-vs-leakyrelu-vs-prelu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# !nvidia-smi\n",
    "# !pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip list\n",
    "# !pip install torch torchvision torchaudio\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "class Cifar10_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10_model, self).__init__()\n",
    "        # 1st convolutional layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=16,\n",
    "                kernel_size=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        # 2nd convolutional layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        # 3rd convolutional layer\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        # output layer\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(64*11*11, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagate x through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Cifar10_model().to(device)\n",
    "\n",
    "print(f'Device: {device}')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577ec50bf7f6b102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:23.517176Z",
     "start_time": "2024-04-23T12:36:23.504177Z"
    },
    "id": "577ec50bf7f6b102"
   },
   "outputs": [],
   "source": [
    "# Lab 10 code\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
    "        '''\n",
    "        :param tolerance: number of epochs that the metric doesn't improve\n",
    "        :param min_delta: minimum improvement\n",
    "        :param mode: 'min' or 'max' to minimize or maximize the metric\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        You should keep these parameters,\n",
    "        define a counter of __call__ falses and the previous best value of metric\n",
    "        '''\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.prev_metric = np.inf if mode == 'min' else -np.inf\n",
    "        self.operation = operator.gt if mode == 'min' else operator.lt\n",
    "\n",
    "\n",
    "    def __call__(self, metric)->bool:\n",
    "        ''' This function should return True if `metric` is not improving for\n",
    "            'tolerance' calls\n",
    "        '''\n",
    "        delta = (metric - self.prev_metric)\n",
    "\n",
    "        if self.operation(delta, self.min_delta):\n",
    "            self.counter +=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.prev_metric = metric\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e4be9bd5ce2daa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:24.032207100Z",
     "start_time": "2024-04-23T12:36:23.521157900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92e4be9bd5ce2daa",
    "outputId": "4605a3b0-2b73-4b14-c5ce-7f8a072e25c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79133\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 code, learning rate scheduler. It's applied to choose the best lr during\n",
    "# the training process\n",
    "from torch.optim import lr_scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# Reference: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "LRs = {\"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3,\n",
    "                                                           patience=10, verbose=True,min_lr=0.001),\n",
    "       \"Step LR\": lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5),\n",
    "       \"Exponent LR\": lr_scheduler.ExponentialLR(optimizer, gamma=0.9),\n",
    "       \"Cyclic LR\":lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.2,\n",
    "                                         cycle_momentum=False, step_size_up=10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9adc761da9ab75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:24.064203900Z",
     "start_time": "2024-04-23T12:36:24.031205500Z"
    },
    "id": "5e9adc761da9ab75"
   },
   "outputs": [],
   "source": [
    "# Lab 10 code train and test functions\n",
    "# ! pip install tqdm\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start_time = time()\n",
    "    correct = 0\n",
    "    iteration = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for data, target in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Get the index of the max log-probability\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        iteration += 1\n",
    "        bar.set_postfix({\"Loss\": format(epoch_loss/iteration, '.6f')})\n",
    "\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    print(f'\\rTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
    "    return epoch_loss, acc\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c4ef0bf4c1c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T12:36:24.065204200Z",
     "start_time": "2024-04-23T12:36:24.048209300Z"
    },
    "id": "56c4ef0bf4c1c0b"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from copy import deepcopy\n",
    "\n",
    "# Define hyperparams\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# Choosing LR\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3,\n",
    "                                                           patience=3, verbose=True, min_lr=0.001)\n",
    "early_stopping = EarlyStopping(tolerance=7, mode='min')\n",
    "best_model_wts = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7321d52b5e5d9919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T13:07:30.171886500Z",
     "start_time": "2024-04-23T12:36:24.065204200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7321d52b5e5d9919",
    "outputId": "42d2457e-98c5-406b-fa34-f7a0225139de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.09it/s, Loss=1.759929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, elapsed time:19.42s\n",
      "Training accuracy 36.992, test accuracy 46.74\n",
      "Training loss 686.3724775314331, test loss 118.79534685611725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.42it/s, Loss=1.474310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2, elapsed time:19.10s\n",
      "Training accuracy 48.036, test accuracy 56.84\n",
      "Training loss 574.9807734489441, test loss 96.85349214076996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 19.55it/s, Loss=1.413511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3, elapsed time:19.95s\n",
      "Training accuracy 50.72, test accuracy 55.63\n",
      "Training loss 551.2694227695465, test loss 99.55080050230026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.47it/s, Loss=1.382717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4, elapsed time:19.05s\n",
      "Training accuracy 52.04, test accuracy 59.76\n",
      "Training loss 539.2594690322876, test loss 94.69908374547958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.48it/s, Loss=1.323562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5, elapsed time:19.05s\n",
      "Training accuracy 54.23, test accuracy 58.37\n",
      "Training loss 516.1890986561775, test loss 100.48216331005096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:20<00:00, 19.08it/s, Loss=1.266691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6, elapsed time:20.45s\n",
      "Training accuracy 55.796, test accuracy 64.07\n",
      "Training loss 494.00963670015335, test loss 82.98117882013321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 19.54it/s, Loss=1.212559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7, elapsed time:19.96s\n",
      "Training accuracy 57.624, test accuracy 63.69\n",
      "Training loss 472.8981402516365, test loss 82.84106373786926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:20<00:00, 19.36it/s, Loss=1.163508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8, elapsed time:20.15s\n",
      "Training accuracy 59.316, test accuracy 63.74\n",
      "Training loss 453.7681695818901, test loss 79.37410682439804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:21<00:00, 18.43it/s, Loss=1.137337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9, elapsed time:21.16s\n",
      "Training accuracy 60.306, test accuracy 66.35\n",
      "Training loss 443.5612493753433, test loss 75.68856209516525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 19.65it/s, Loss=1.103366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10, elapsed time:19.85s\n",
      "Training accuracy 61.35, test accuracy 68.75\n",
      "Training loss 430.3127566576004, test loss 70.1827597618103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.61it/s, Loss=1.072607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11, elapsed time:18.92s\n",
      "Training accuracy 62.386, test accuracy 69.58\n",
      "Training loss 418.31668573617935, test loss 69.09827655553818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.54it/s, Loss=1.051317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12, elapsed time:18.99s\n",
      "Training accuracy 63.212, test accuracy 70.49\n",
      "Training loss 410.0134735107422, test loss 66.27765420079231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.33it/s, Loss=1.036140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13, elapsed time:19.19s\n",
      "Training accuracy 63.832, test accuracy 71.59\n",
      "Training loss 404.0945081114769, test loss 66.14741206169128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.51it/s, Loss=1.003804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14, elapsed time:19.02s\n",
      "Training accuracy 64.888, test accuracy 71.59\n",
      "Training loss 391.48339158296585, test loss 62.793825507164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.24it/s, Loss=0.996574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15, elapsed time:19.28s\n",
      "Training accuracy 65.164, test accuracy 72.79\n",
      "Training loss 388.66377317905426, test loss 63.978801250457764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.51it/s, Loss=0.988772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16, elapsed time:19.02s\n",
      "Training accuracy 65.524, test accuracy 72.6\n",
      "Training loss 385.62103176116943, test loss 62.828800678253174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.64it/s, Loss=0.976183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17, elapsed time:18.90s\n",
      "Training accuracy 65.884, test accuracy 72.15\n",
      "Training loss 380.7114289999008, test loss 65.20055288076401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.55it/s, Loss=0.967950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18, elapsed time:18.98s\n",
      "Training accuracy 66.416, test accuracy 72.93\n",
      "Training loss 377.5005541443825, test loss 63.37118446826935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.58it/s, Loss=0.951899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19, elapsed time:18.95s\n",
      "Training accuracy 66.85, test accuracy 75.03\n",
      "Training loss 371.2407942414284, test loss 59.16467934846878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.54it/s, Loss=0.954532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20, elapsed time:18.99s\n",
      "Training accuracy 66.63, test accuracy 73.63\n",
      "Training loss 372.2675054073334, test loss 60.28387135267258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.42it/s, Loss=0.949706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21, elapsed time:19.10s\n",
      "Training accuracy 67.13, test accuracy 73.52\n",
      "Training loss 370.3854983150959, test loss 61.892553210258484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.54it/s, Loss=0.932456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22, elapsed time:18.99s\n",
      "Training accuracy 67.552, test accuracy 73.23\n",
      "Training loss 363.6579886674881, test loss 61.370517402887344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.39it/s, Loss=0.932453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23, elapsed time:19.13s\n",
      "Training accuracy 67.508, test accuracy 73.63\n",
      "Training loss 363.6568278670311, test loss 59.50195360183716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.55it/s, Loss=0.915039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24, elapsed time:18.98s\n",
      "Training accuracy 68.238, test accuracy 74.43\n",
      "Training loss 356.8652173280716, test loss 60.36752462387085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.48it/s, Loss=0.914151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25, elapsed time:19.05s\n",
      "Training accuracy 68.656, test accuracy 75.83\n",
      "Training loss 356.5188881158829, test loss 56.36476814746857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.51it/s, Loss=0.908252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26, elapsed time:19.02s\n",
      "Training accuracy 68.416, test accuracy 74.18\n",
      "Training loss 354.218409717083, test loss 58.37699991464615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.57it/s, Loss=0.906390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27, elapsed time:18.97s\n",
      "Training accuracy 68.554, test accuracy 75.89\n",
      "Training loss 353.49191880226135, test loss 59.52516037225723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.39it/s, Loss=0.891354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28, elapsed time:19.13s\n",
      "Training accuracy 69.11, test accuracy 75.8\n",
      "Training loss 347.6280128955841, test loss 56.995022654533386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.44it/s, Loss=0.896702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29, elapsed time:19.08s\n",
      "Training accuracy 68.68, test accuracy 75.16\n",
      "Training loss 349.71378725767136, test loss 61.40827256441116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.56it/s, Loss=0.892294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30, elapsed time:18.97s\n",
      "Training accuracy 69.098, test accuracy 76.55\n",
      "Training loss 347.99453753232956, test loss 56.24958062171936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.50it/s, Loss=0.889331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31, elapsed time:19.02s\n",
      "Training accuracy 69.264, test accuracy 75.89\n",
      "Training loss 346.8389399051666, test loss 57.61089268326759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.56it/s, Loss=0.875154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32, elapsed time:18.97s\n",
      "Training accuracy 69.52, test accuracy 75.66\n",
      "Training loss 341.3101750612259, test loss 56.38264226913452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.09it/s, Loss=0.871805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 33, elapsed time:19.42s\n",
      "Training accuracy 69.926, test accuracy 75.63\n",
      "Training loss 340.00379210710526, test loss 55.783997893333435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.29it/s, Loss=0.868242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34, elapsed time:19.23s\n",
      "Training accuracy 69.858, test accuracy 74.13\n",
      "Training loss 338.61432310938835, test loss 58.233831226825714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.42it/s, Loss=0.881464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35, elapsed time:19.10s\n",
      "Training accuracy 69.624, test accuracy 76.23\n",
      "Training loss 343.7708355784416, test loss 54.443907141685486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.55it/s, Loss=0.866576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36, elapsed time:18.98s\n",
      "Training accuracy 69.968, test accuracy 76.35\n",
      "Training loss 337.9647201895714, test loss 54.750521540641785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.54it/s, Loss=0.862750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37, elapsed time:18.99s\n",
      "Training accuracy 70.072, test accuracy 76.21\n",
      "Training loss 336.47255742549896, test loss 54.901883363723755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.43it/s, Loss=0.855293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38, elapsed time:19.09s\n",
      "Training accuracy 70.472, test accuracy 76.46\n",
      "Training loss 333.56410759687424, test loss 57.83837714791298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.48it/s, Loss=0.866643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39, elapsed time:19.04s\n",
      "Training accuracy 70.236, test accuracy 76.19\n",
      "Training loss 337.990831553936, test loss 57.16347414255142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.52it/s, Loss=0.844675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40, elapsed time:19.00s\n",
      "Training accuracy 70.666, test accuracy 77.86\n",
      "Training loss 329.4233305454254, test loss 51.673425287008286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.27it/s, Loss=0.852303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41, elapsed time:19.24s\n",
      "Training accuracy 70.502, test accuracy 76.33\n",
      "Training loss 332.3979831337929, test loss 53.6647364795208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.41it/s, Loss=0.850912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42, elapsed time:19.11s\n",
      "Training accuracy 70.908, test accuracy 75.95\n",
      "Training loss 331.8557026386261, test loss 55.18781340122223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.42it/s, Loss=0.835919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43, elapsed time:19.10s\n",
      "Training accuracy 71.216, test accuracy 75.66\n",
      "Training loss 326.0084396004677, test loss 62.09466230869293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.47it/s, Loss=0.842795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44, elapsed time:19.05s\n",
      "Training accuracy 70.964, test accuracy 75.77\n",
      "Training loss 328.69006472826004, test loss 57.252180606126785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.38it/s, Loss=0.835392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45, elapsed time:19.14s\n",
      "Training accuracy 71.276, test accuracy 77.66\n",
      "Training loss 325.8028270602226, test loss 55.63011571764946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:19<00:00, 20.36it/s, Loss=0.828060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46, elapsed time:19.16s\n",
      "Training accuracy 71.376, test accuracy 77.8\n",
      "Training loss 322.94333946704865, test loss 53.88644599914551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [00:18<00:00, 20.54it/s, Loss=0.825159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47, elapsed time:18.99s\n",
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Lab10 code, training function\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "\n",
    "def training(writing=False):\n",
    "    if writing:\n",
    "        writer = SummaryWriter(log_dir='runs/model')\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
    "        # Update learning rate if needed\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        test_loss, test_acc = test(model, device, test_data_loader, criterion)\n",
    "        # Terminate training if loss stopped to decrease\n",
    "        if early_stopping(test_loss):\n",
    "            print('\\nEarly stopping\\n')\n",
    "            break\n",
    "        # Deep copy the weight of model if its accuracy is the best for now\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if writing:\n",
    "            writer.add_scalars('Loss',\n",
    "                            {\n",
    "                                'train': train_loss,\n",
    "                                'test': test_loss\n",
    "                            },\n",
    "                            epoch)\n",
    "\n",
    "            writer.add_scalars('Accuracy',\n",
    "                            {\n",
    "                                'train': train_acc,\n",
    "                                'test': test_acc\n",
    "                            },\n",
    "                            epoch)\n",
    "        else:\n",
    "            print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
    "            print(f\"Training loss {train_loss}, test loss {test_loss}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_task2.pt\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), \"best_model_task2.pt\")\n",
    "    if writing:\n",
    "        writer.close()\n",
    "# Train and save model that performs the best\n",
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3ac01dc0abaea",
   "metadata": {
    "id": "53a3ac01dc0abaea"
   },
   "source": [
    "Let's evaluate this model on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a881a6eca2280cbf",
   "metadata": {
    "id": "a881a6eca2280cbf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's evaluate cnn model on test data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Upload, normalize, and convert to tensor test images\n",
    "test_images = np.load('task_2_test_images.npy')\n",
    "# we need to reshape the inputs to pass it to the neural network\n",
    "test_images_reshaped = np.moveaxis(test_images, 3, 1)\n",
    "test_labels = np.load('task_2_test_labels.npy')\n",
    "test_images_tensor = torch.tensor(test_images_reshaped, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "test_images_tensor /= 255\n",
    "#Reference https://www.geeksforgeeks.org/save-and-load-models-in-pytorch/\n",
    "# Let's load CNN model\n",
    "cnn_model = Cifar10_model()\n",
    "cnn_model.load_state_dict(torch.load('best_model_task2.pt'))\n",
    "output = cnn_model(test_images_tensor)\n",
    "pred = output.argmax(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ZnbE7lg_Gy6P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnbE7lg_Gy6P",
    "outputId": "50c3b611-45ad-42c7-e786-6113da6c0f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC-ROC score for CNN model: 0.8002385042738105\n"
     ]
    }
   ],
   "source": [
    "# Let's print AUC-ROC score\n",
    "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "# We need to perform it due to multi-class classification\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import metrics\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(test_labels)\n",
    "test_labels_bin = lb.transform(test_labels)\n",
    "pred_bin = lb.transform(pred.numpy())\n",
    "auc_score_array = metrics.roc_auc_score(test_labels_bin, pred_bin, average=None, multi_class='ovo')\n",
    "print(\"Mean AUC-ROC score for CNN model:\", np.mean(auc_score_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560433d-2ab5-4b26-87a0-feee41a144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build and finetune pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b3ba5f-50d3-4b10-a3ae-75f821206024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79133\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\79133\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 10 self-practise code\n",
    "import torchvision.models as models\n",
    "batch_size = 32\n",
    "# Create pretrained model\n",
    "pretrained_model =  models.resnet50(pretrained=True)\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e69c29f-fabe-4422-b6ca-267e899ea103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input layer according to cifar10\n",
    "pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "# replace number of output classes\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56719058-c12c-4fad-8ce7-a67c3e278cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79133\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 self-practise code\n",
    "from torch.optim import SGD\n",
    "from torch.optim import lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "# Again, define hyperparameters (we don't need large number of epochs)\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(pretrained_model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2, verbose=True, min_lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa77e5ce-48a8-46a1-8075-e8a33ee85bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.50it/s, Loss=1.251244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, elapsed time:60.03s\n",
      "Training accuracy 55.544, test accuracy 70.62\n",
      "Training loss 487.9851709008217, test loss 74.36300724744797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.48it/s, Loss=0.735274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, elapsed time:60.18s\n",
      "Training accuracy 74.116, test accuracy 80.66\n",
      "Training loss 286.7569063901901, test loss 43.37174245715141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.49it/s, Loss=0.585380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2, elapsed time:60.11s\n",
      "Training accuracy 79.494, test accuracy 83.07\n",
      "Training loss 228.2980423271656, test loss 38.59309211373329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.48it/s, Loss=0.494268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3, elapsed time:60.21s\n",
      "Training accuracy 82.5, test accuracy 85.69\n",
      "Training loss 192.76445445418358, test loss 32.54959836602211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.47it/s, Loss=0.430460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4, elapsed time:60.28s\n",
      "Training accuracy 84.752, test accuracy 87.2\n",
      "Training loss 167.8792096376419, test loss 29.644398786127567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.48it/s, Loss=0.382085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5, elapsed time:60.22s\n",
      "Training accuracy 86.732, test accuracy 87.97\n",
      "Training loss 149.0130054950714, test loss 27.017413392663002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:01<00:00,  6.38it/s, Loss=0.347548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6, elapsed time:61.18s\n",
      "Training accuracy 87.662, test accuracy 86.95\n",
      "Training loss 135.54390709102154, test loss 30.403337463736534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.48it/s, Loss=0.324499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7, elapsed time:60.20s\n",
      "Training accuracy 88.576, test accuracy 88.74\n",
      "Training loss 126.55458089709282, test loss 27.252283424139023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.47it/s, Loss=0.296667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8, elapsed time:60.25s\n",
      "Training accuracy 89.296, test accuracy 89.19\n",
      "Training loss 115.70014302432537, test loss 25.384615778923035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 390/390 [01:00<00:00,  6.46it/s, Loss=0.279331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9, elapsed time:60.38s\n",
      "Training accuracy 90.026, test accuracy 89.61\n",
      "Training loss 108.9392597079277, test loss 24.53753024339676\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 code\n",
    "# Train finetunde model\n",
    "def training():\n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss, train_acc = train(pretrained_model, device, train_data_loader, criterion, optimizer, epoch)\n",
    "        # Update learning rate if needed\n",
    "        scheduler.step(train_loss)\n",
    "        test_loss, test_acc = test(pretrained_model, device, test_data_loader, criterion)\n",
    "        print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
    "        print(f\"Training loss {train_loss}, test loss {test_loss}\")\n",
    "\n",
    "    torch.save(pretrained_model.state_dict(), \"model_finetuned.pt\")\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69bff22-1d20-46d3-b36c-efebcd7fb032",
   "metadata": {
    "id": "be7e7d40f72bd760"
   },
   "source": [
    "\n",
    "\n",
    "Model size of resnet50 is much bigger that for my CNN model. It requires more memory, more CPU/GPU power, takes more time to learn (we can see it in the training process (number of iterations per second)). Inference speed of CNN implemented from scratch is much faster as I have less layers and less complex structure than resNet50. Resnet50 is a very large model for complex classification tasks (10000 classes), so usually it is used for more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84591496-5e0c-4b9e-b7e7-0481d54dead5",
   "metadata": {},
   "source": [
    "Let's obtain predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82dc2c17-4ea2-4f7c-b9d7-a7bf302f89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "test_images = np.load('task_2_test_images.npy')\n",
    "test_labels = np.load('task_2_test_labels.npy')\n",
    "# apply same transofrm as for cifar10 dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=2),# add zeros to save initial size of an image after transformations\n",
    "    transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "    transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transformed_images = []\n",
    "# Process all images one by one (I asked chatGPT to help me with this for loop. I tried to process images alltogether, but it suggests to process it one by one if for loop)\n",
    "for image_array in test_images:\n",
    "    # Convert numpy array to PIL image\n",
    "    image_pil = Image.fromarray(np.uint8(image_array))\n",
    "\n",
    "    # Apply transformations\n",
    "    transformed_image = train_transforms(image_pil)\n",
    "\n",
    "    # Convert transformed image back to numpy array\n",
    "    transformed_image_array = np.array(transformed_image)\n",
    "\n",
    "    # Append transformed image to list\n",
    "    transformed_images.append(transformed_image_array)\n",
    "# Convert list of transformed images to numpy array\n",
    "transformed_images_array = np.array(transformed_images)\n",
    "# Convert to tensor\n",
    "test_images_tensor = torch.tensor(transformed_images_array, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "# Create Tensor test dataset\n",
    "test_dataset_transformed = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "# Create test data loader\n",
    "test_dataset_loader = DataLoader(test_dataset_transformed, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ed72290-4be6-47ab-a85a-86003147c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average AUC: 0.9696829625639631\n"
     ]
    }
   ],
   "source": [
    "# Calculate AUC-ROC score for pretrained model\n",
    "from sklearn import metrics\n",
    "pretrained_model.eval()\n",
    "test_labels_ans = []\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_dataset_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = pretrained_model(data)\n",
    "        probabilities = nn.functional.softmax(output, dim=1)\n",
    "        test_labels_ans.extend(target.cpu().numpy())\n",
    "        test_predictions.extend(probabilities.cpu().numpy())\n",
    "test_labels_ans_onehot = np.eye(10)[test_labels_ans]\n",
    "micro_auc = metrics.roc_auc_score(test_labels_ans_onehot, np.array(test_predictions), average='micro')\n",
    "print(f\"Micro-average AUC: {micro_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0882279-ac65-4e44-90c1-0e295067714a",
   "metadata": {},
   "source": [
    "My custom CNN shows not bad performance on test dataset given for the task, but AUC-ROC score of the complex model is almost 1. It means that pretrained model performs well on test data and this is reasonable to use such approach.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "test_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
